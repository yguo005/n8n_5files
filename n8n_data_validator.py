#!/usr/bin/env python3
"""
n8n Data Quality Validator & Checkpoint
Validates preprocessed questionnaire data before trend analysis or LLM processing
Use this as a checkpoint node between Preprocessor and Trend Analyzer
"""

from datetime import datetime
from typing import Dict, List, Any, Optional

def parse_date(date_str: str) -> Optional[datetime]:
    """Parse date string to datetime object"""
    if not date_str:
        return None
    try:
        return datetime.fromisoformat(date_str.replace('Z', '+00:00'))
    except:
        try:
            return datetime.strptime(date_str, '%Y-%m-%d')
        except:
            return None

def validate_preprocessed_data(items: List[Dict]) -> Dict[str, Any]:
    """
    Comprehensive validation of preprocessed questionnaire data
    
    Args:
        items: List of preprocessed items from the questionnaire preprocessor
        
    Returns:
        Validation report with data quality metrics, warnings, and pass/fail status
    """
    
    validation_report = {
        "status": "PASS",  # Will change to FAIL or WARNING if issues found
        "total_items": len(items),
        "validation_checks": {},
        "data_quality_metrics": {},
        "warnings": [],
        "errors": [],
        "recommendations": []
    }
    
    if not items:
        validation_report["status"] = "FAIL"
        validation_report["errors"].append("No data received - empty input")
        return validation_report
    
    # Extract json data from items
    data_items = []
    for item in items:
        json_data = item.get('json', {})
        if json_data:
            data_items.append(json_data)
    
    if not data_items:
        validation_report["status"] = "FAIL"
        validation_report["errors"].append("No valid JSON data found in items")
        return validation_report
    
    # =========================================================================
    # CHECK 1: Required Fields Validation
    # =========================================================================
    required_fields = ['questionnaire', 'timepoint', 'raw_total', 'severity']
    optional_but_recommended = ['date', 'clinical_flags', 'derived', 'responses']
    
    items_with_all_required = 0
    items_with_dates = 0
    items_with_derived = 0
    items_with_responses = 0
    missing_fields_by_item = []
    
    for idx, item in enumerate(data_items):
        missing = [field for field in required_fields if field not in item or not item[field]]
        if not missing:
            items_with_all_required += 1
        else:
            missing_fields_by_item.append({
                'item_index': idx,
                'questionnaire': item.get('questionnaire', 'Unknown'),
                'missing_fields': missing
            })
        
        if item.get('date'):
            items_with_dates += 1
        if item.get('derived') and isinstance(item['derived'], dict) and item['derived']:
            items_with_derived += 1
        if item.get('responses'):
            items_with_responses += 1
    
    validation_report["validation_checks"]["required_fields"] = {
        "pass": len(missing_fields_by_item) == 0,
        "items_with_all_required": items_with_all_required,
        "items_missing_fields": len(missing_fields_by_item),
        "details": missing_fields_by_item[:5]  # Show first 5 only
    }
    
    if missing_fields_by_item:
        validation_report["status"] = "FAIL"
        validation_report["errors"].append(
            f"{len(missing_fields_by_item)} items missing required fields: {required_fields}"
        )
    
    # =========================================================================
    # CHECK 2: Date/Timepoint Quality
    # =========================================================================
    valid_dates = []
    valid_timepoints = []
    invalid_dates = []
    
    for idx, item in enumerate(data_items):
        # Check dates
        date_str = item.get('date', '')
        date_obj = parse_date(date_str)
        if date_obj:
            valid_dates.append(date_obj)
        elif date_str and date_str.strip():
            invalid_dates.append({
                'item_index': idx,
                'questionnaire': item.get('questionnaire', 'Unknown'),
                'invalid_date': date_str
            })
        
        # Check timepoints
        timepoint = item.get('timepoint', 0)
        if timepoint and timepoint > 0:
            valid_timepoints.append(timepoint)
    
    # Determine date range
    date_range_info = None
    if valid_dates:
        valid_dates.sort()
        date_range_info = {
            "earliest": valid_dates[0].strftime('%Y-%m-%d'),
            "latest": valid_dates[-1].strftime('%Y-%m-%d'),
            "span_days": (valid_dates[-1] - valid_dates[0]).days,
            "total_with_dates": len(valid_dates)
        }
    
    # Determine timepoint range
    timepoint_range_info = None
    if valid_timepoints:
        valid_timepoints_sorted = sorted(valid_timepoints)
        timepoint_range_info = {
            "earliest": valid_timepoints_sorted[0],
            "latest": valid_timepoints_sorted[-1],
            "unique_timepoints": len(set(valid_timepoints)),
            "total_with_timepoints": len(valid_timepoints)
        }
    
    validation_report["validation_checks"]["date_timepoint_quality"] = {
        "items_with_dates": len(valid_dates),
        "items_with_valid_timepoints": len(valid_timepoints),
        "invalid_dates": len(invalid_dates),
        "date_range": date_range_info,
        "timepoint_range": timepoint_range_info
    }
    
    # Warnings for date/timepoint issues
    if len(valid_dates) < len(data_items):
        pct = (len(valid_dates) / len(data_items)) * 100
        validation_report["warnings"].append(
            f"Only {len(valid_dates)}/{len(data_items)} ({pct:.1f}%) items have valid dates"
        )
        if pct < 50:
            validation_report["recommendations"].append(
                "Consider adding dates to more items for better trend analysis"
            )
    
    if invalid_dates:
        validation_report["warnings"].append(
            f"{len(invalid_dates)} items have invalid date formats"
        )
    
    # =========================================================================
    # CHECK 3: Questionnaire Distribution & Grouping
    # =========================================================================
    questionnaire_groups = {}
    
    for item in data_items:
        q_name = item.get('questionnaire', 'Unknown')
        if q_name not in questionnaire_groups:
            questionnaire_groups[q_name] = {
                'count': 0,
                'timepoints': set(),
                'has_dates': 0,
                'has_derived': 0,
                'total_scores': []
            }
        
        questionnaire_groups[q_name]['count'] += 1
        questionnaire_groups[q_name]['timepoints'].add(item.get('timepoint', 0))
        if item.get('date'):
            questionnaire_groups[q_name]['has_dates'] += 1
        if item.get('derived') and item['derived']:
            questionnaire_groups[q_name]['has_derived'] += 1
        questionnaire_groups[q_name]['total_scores'].append(item.get('raw_total', 0))
    
    # Convert sets to counts for JSON serialization
    questionnaire_summary = {}
    questionnaires_ready_for_trends = []
    questionnaires_insufficient_data = []
    
    for q_name, info in questionnaire_groups.items():
        timepoint_count = len(info['timepoints'])
        questionnaire_summary[q_name] = {
            'total_assessments': info['count'],
            'unique_timepoints': timepoint_count,
            'assessments_with_dates': info['has_dates'],
            'assessments_with_derived': info['has_derived'],
            'score_range': {
                'min': min(info['total_scores']) if info['total_scores'] else 0,
                'max': max(info['total_scores']) if info['total_scores'] else 0
            }
        }
        
        # Check if ready for trend analysis (needs 2+ timepoints)
        if timepoint_count >= 2:
            questionnaires_ready_for_trends.append(q_name)
        else:
            questionnaires_insufficient_data.append(q_name)
    
    validation_report["data_quality_metrics"]["questionnaire_distribution"] = {
        "total_questionnaires": len(questionnaire_groups),
        "questionnaires_ready_for_trends": len(questionnaires_ready_for_trends),
        "questionnaires_insufficient_data": len(questionnaires_insufficient_data),
        "summary": questionnaire_summary
    }
    
    # Warnings for questionnaires with insufficient data
    if questionnaires_insufficient_data:
        validation_report["warnings"].append(
            f"{len(questionnaires_insufficient_data)} questionnaire(s) have only 1 timepoint - cannot analyze trends: {', '.join(questionnaires_insufficient_data)}"
        )
        validation_report["recommendations"].append(
            "Collect data at multiple timepoints to enable trend analysis"
        )
    
    # =========================================================================
    # CHECK 4: Derived Data Completeness
    # =========================================================================
    items_with_empty_derived = []
    items_with_scale_info = 0
    items_with_interpretations = 0
    
    for idx, item in enumerate(data_items):
        derived = item.get('derived', {})
        if not derived or not isinstance(derived, dict):
            items_with_empty_derived.append({
                'item_index': idx,
                'questionnaire': item.get('questionnaire', 'Unknown'),
                'timepoint': item.get('timepoint', '?')
            })
        else:
            if derived.get('scale'):
                items_with_scale_info += 1
            if derived.get('interpretations'):
                items_with_interpretations += 1
    
    validation_report["validation_checks"]["derived_data_quality"] = {
        "items_with_derived": items_with_derived,
        "items_with_empty_derived": len(items_with_empty_derived),
        "items_with_scale_info": items_with_scale_info,
        "items_with_interpretations": items_with_interpretations
    }
    
    if items_with_empty_derived:
        if len(items_with_empty_derived) == len(data_items):
            validation_report["status"] = "FAIL"
            validation_report["errors"].append(
                "All items have empty 'derived' dictionary - preprocessor may not be working correctly"
            )
        else:
            validation_report["warnings"].append(
                f"{len(items_with_empty_derived)} items have empty 'derived' data"
            )
    
    # =========================================================================
    # CHECK 5: Score Validity
    # =========================================================================
    items_with_zero_scores = []
    items_with_negative_scores = []
    
    for idx, item in enumerate(data_items):
        raw_total = item.get('raw_total', 0)
        if raw_total == 0:
            items_with_zero_scores.append({
                'item_index': idx,
                'questionnaire': item.get('questionnaire', 'Unknown'),
                'timepoint': item.get('timepoint', '?')
            })
        elif raw_total < 0:
            items_with_negative_scores.append({
                'item_index': idx,
                'questionnaire': item.get('questionnaire', 'Unknown'),
                'raw_total': raw_total
            })
    
    validation_report["validation_checks"]["score_validity"] = {
        "items_with_zero_scores": len(items_with_zero_scores),
        "items_with_negative_scores": len(items_with_negative_scores)
    }
    
    if items_with_negative_scores:
        validation_report["status"] = "FAIL"
        validation_report["errors"].append(
            f"{len(items_with_negative_scores)} items have negative scores (invalid)"
        )
    
    if len(items_with_zero_scores) > len(data_items) * 0.5:
        validation_report["warnings"].append(
            f"{len(items_with_zero_scores)} items have zero scores - verify this is expected"
        )
    
    # =========================================================================
    # CHECK 6: Trend Analysis Readiness
    # =========================================================================
    can_analyze_trends = False
    sort_method = None
    trend_readiness_issues = []
    
    if len(valid_dates) >= 2:
        can_analyze_trends = True
        sort_method = "date_primary"
    elif len(valid_timepoints) >= 2:
        can_analyze_trends = True
        sort_method = "timepoint_only"
        trend_readiness_issues.append("No dates available - will use timepoint ordering only")
    else:
        trend_readiness_issues.append("Need at least 2 items with dates or timepoints for trend analysis")
    
    validation_report["data_quality_metrics"]["trend_analysis_readiness"] = {
        "ready_for_trend_analysis": can_analyze_trends,
        "recommended_sort_method": sort_method,
        "issues": trend_readiness_issues,
        "questionnaires_with_trends": len(questionnaires_ready_for_trends),
        "total_questionnaires": len(questionnaire_groups)
    }
    
    if not can_analyze_trends:
        validation_report["warnings"].append(
            "Insufficient data for trend analysis - need at least 2 timepoints with dates or timepoint markers"
        )
    
    # =========================================================================
    # CHECK 7: Clinical Flags Review
    # =========================================================================
    items_with_clinical_flags = 0
    total_clinical_flags = 0
    critical_flags = []
    
    for item in data_items:
        flags = item.get('clinical_flags', [])
        if flags:
            items_with_clinical_flags += 1
            total_clinical_flags += len(flags)
            
            # Identify critical flags (severe, abnormal, etc.)
            for flag in flags:
                flag_lower = flag.lower()
                if any(keyword in flag_lower for keyword in ['severe', 'abnormal', 'high risk', 'clinical attention']):
                    critical_flags.append({
                        'questionnaire': item.get('questionnaire', 'Unknown'),
                        'timepoint': item.get('timepoint', '?'),
                        'flag': flag
                    })
    
    validation_report["data_quality_metrics"]["clinical_flags_summary"] = {
        "items_with_flags": items_with_clinical_flags,
        "total_flags": total_clinical_flags,
        "critical_flags": len(critical_flags),
        "critical_flags_details": critical_flags[:10]  # Show first 10
    }
    
    if critical_flags:
        validation_report["recommendations"].append(
            f"{len(critical_flags)} critical clinical flags detected - review before LLM processing"
        )
    
    # =========================================================================
    # FINAL STATUS DETERMINATION
    # =========================================================================
    if validation_report["errors"]:
        validation_report["status"] = "FAIL"
        validation_report["recommendations"].append(
            "Fix errors before proceeding to trend analysis or LLM processing"
        )
    elif validation_report["warnings"]:
        if validation_report["status"] != "FAIL":
            validation_report["status"] = "WARNING"
        validation_report["recommendations"].append(
            "Review warnings - data may still be usable but quality could be improved"
        )
    
    # Add summary
    validation_report["summary"] = {
        "total_items": len(data_items),
        "items_with_all_required_fields": items_with_all_required,
        "questionnaires_analyzed": len(questionnaire_groups),
        "ready_for_trend_analysis": can_analyze_trends,
        "critical_issues": len(validation_report["errors"]),
        "warnings": len(validation_report["warnings"]),
        "status": validation_report["status"]
    }
    
    return validation_report

def format_validation_report(validation: Dict[str, Any]) -> str:
    """Format validation report as human-readable text"""
    lines = []
    lines.append("=" * 70)
    lines.append("DATA QUALITY VALIDATION REPORT")
    lines.append("=" * 70)
    lines.append(f"Status: {validation['status']}")
    lines.append(f"Total Items: {validation['total_items']}")
    lines.append("")
    
    # Summary
    if validation.get('summary'):
        lines.append("SUMMARY:")
        for key, value in validation['summary'].items():
            lines.append(f"  {key}: {value}")
        lines.append("")
    
    # Errors
    if validation.get('errors'):
        lines.append("ERRORS:")
        for error in validation['errors']:
            lines.append(f"  ❌ {error}")
        lines.append("")
    
    # Warnings
    if validation.get('warnings'):
        lines.append("WARNINGS:")
        for warning in validation['warnings']:
            lines.append(f"  ⚠️  {warning}")
        lines.append("")
    
    # Recommendations
    if validation.get('recommendations'):
        lines.append("RECOMMENDATIONS:")
        for rec in validation['recommendations']:
            lines.append(f"  💡 {rec}")
        lines.append("")
    
    lines.append("=" * 70)
    return "\n".join(lines)

# =============================================================================
# n8n CODE NODE EXECUTION (Direct execution - no function wrappers)
# =============================================================================
# Note: In n8n, 'items' is a global variable provided by the platform
# This validator node should be placed AFTER the Preprocessor and BEFORE the Trend Analyzer

# Check if running in n8n environment
if 'items' in globals():
    try:
        # Debug: Log what we received from preprocessor
        print(f"🔍 VALIDATOR: Received {len(items)} items from preprocessor")
        
        # Run validation
        validation_result = validate_preprocessed_data(items)
        
        # Print formatted report
        print("\n" + format_validation_report(validation_result))
        
        # Determine what to output based on validation status
        if validation_result['status'] == 'FAIL':
            print(f"❌ VALIDATION FAILED: {len(validation_result['errors'])} critical error(s) found")
            print("🛑 Stopping workflow - fix errors before proceeding")
            
            # Return validation report only (not the data)
            return [{'json': {
                'validation_status': 'FAIL',
                'validation_report': validation_result,
                'data_passed': False,
                'message': 'Data quality check failed - see validation_report for details'
            }}]
        
        elif validation_result['status'] == 'WARNING':
            print(f"⚠️  VALIDATION PASSED WITH WARNINGS: {len(validation_result['warnings'])} warning(s)")
            print("✅ Proceeding with data - review warnings")
            
            # Return both validation report AND original data
            return [{'json': {
                'validation_status': 'WARNING',
                'validation_report': validation_result,
                'data_passed': True,
                'message': 'Data quality check passed with warnings',
                'processed_data': [item for item in items]  # Pass through original data
            }}]
        
        else:  # PASS
            print(f"✅ VALIDATION PASSED: All checks successful")
            print(f"   → {validation_result['summary']['questionnaires_analyzed']} questionnaires validated")
            print(f"   → Ready for trend analysis: {validation_result['data_quality_metrics']['trend_analysis_readiness']['ready_for_trend_analysis']}")
            
            # Return both validation report AND original data
            return [{'json': {
                'validation_status': 'PASS',
                'validation_report': validation_result,
                'data_passed': True,
                'message': 'Data quality check passed - ready for trend analysis',
                'processed_data': [item for item in items]  # Pass through original data
            }}]

    except Exception as e:
        # Return detailed error information for n8n debugging
        import traceback
        
        error_details = {
            'error_message': str(e),
            'error_type': type(e).__name__,
            'input_items_count': len(items) if 'items' in globals() else 0,
            'traceback': traceback.format_exc(),
            'debug_info': {
                'items_available': 'items' in globals(),
                'items_type': type(items).__name__ if 'items' in globals() else 'undefined',
                'help': 'Check that this Validator node is connected after the Questionnaire Preprocessor node'
            }
        }
        
        print(f"❌ VALIDATOR ERROR: {str(e)}")
        print(f"🔍 VALIDATOR DEBUG: Error details logged in output")
        
        # Return error as JSON for next node
        return [{'json': error_details}]

