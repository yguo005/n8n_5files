{
  "name": "My workflow",
  "nodes": [
    {
      "parameters": {},
      "type": "n8n-nodes-base.manualTrigger",
      "typeVersion": 1,
      "position": [
        0,
        0
      ],
      "id": "2dce982f-8a8b-4529-9fc6-0e00af2170aa",
      "name": "When clicking ‘Execute workflow’"
    },
    {
      "parameters": {
        "operation": "download",
        "fileId": {
          "__rl": true,
          "value": "1rUJOquBjN3iYIi7pWm1-jJ9B4c0OyYBo",
          "mode": "list",
          "cachedResultName": "5_profiles_Nov4.xlsx",
          "cachedResultUrl": "https://docs.google.com/spreadsheets/d/1rUJOquBjN3iYIi7pWm1-jJ9B4c0OyYBo/edit?usp=drivesdk&ouid=115828094866410806407&rtpof=true&sd=true"
        },
        "options": {}
      },
      "type": "n8n-nodes-base.googleDrive",
      "typeVersion": 3,
      "position": [
        208,
        0
      ],
      "id": "2feb774d-fba6-4530-b1d3-591f6f8dc4f5",
      "name": "Download file",
      "credentials": {
        "googleDriveOAuth2Api": {
          "id": "XSEoA4AgdnRiA8kV",
          "name": "Google Drive account"
        }
      }
    },
    {
      "parameters": {
        "operation": "xlsx",
        "options": {
          "sheetName": "Depressionless than mild anxiet"
        }
      },
      "type": "n8n-nodes-base.extractFromFile",
      "typeVersion": 1,
      "position": [
        416,
        0
      ],
      "id": "a1cbe7ad-2d74-40da-b276-21acc661ba1a",
      "name": "Extract from File"
    },
    {
      "parameters": {
        "language": "python",
        "pythonCode": "# n8n Questionnaire Preprocessor (Python)\n# Converts raw questionnaire data into structured, interpreted results for LLM processing\n\nimport json\nimport re\nfrom datetime import datetime, timedelta\nfrom typing import Dict, List, Any, Optional\nimport math\n\n# Clinical cut-offs and scoring information from reference table\nQUESTIONNAIRE_CUTOFFS = {\n    \"phq-9\": {\n        \"scale_range\": \"0-27\",\n        \"direction\": \"higher worse\", \n        \"cutoffs\": {\"mild\": 5, \"moderate\": 10, \"moderately_severe\": 15, \"severe\": 20},\n        \"clinical_flag\": {\"threshold\": 10, \"meaning\": \"likely MDD\"}\n    },\n    \"gad-7\": {\n        \"scale_range\": \"0-21\", \n        \"direction\": \"higher worse\",\n        \"cutoffs\": {\"mild\": 5, \"moderate\": 10, \"severe\": 15}\n    },\n    \"who-5\": {\n        \"scale_range\": \"0-25 raw, 0-100 index\",\n        \"direction\": \"lower worse\", \n        \"cutoffs\": {\"poor_wellbeing\": 50, \"depression_risk\": 28},\n        \"transformation\": \"multiply raw by 4\"\n    },\n    \"promis-depression\": {\n        \"scale_range\": \"T-score (mean 50, SD 10)\",\n        \"direction\": \"higher worse\",\n        \"cutoffs\": {\"normal\": 55, \"mild\": 60, \"moderate\": 70, \"severe\": 70}\n    },\n    \"promis-anxiety\": {\n        \"scale_range\": \"T-score (mean 50, SD 10)\",\n        \"direction\": \"higher worse\",\n        \"cutoffs\": {\"normal\": 55, \"mild\": 60, \"moderate\": 70, \"severe\": 70}\n    },\n    \"promis-life\": {\n        \"scale_range\": \"T-score (mean 50, SD 10)\",\n        \"direction\": \"lower worse\",\n        \"cutoffs\": {\"poor\": 40, \"below_average\": 45}\n    },\n    \"ces-dc\": {\n        \"scale_range\": \"0-60\",\n        \"direction\": \"higher worse\",\n        \"cutoffs\": {\"depression_risk\": 15}\n    },\n    \"scared\": {\n        \"scale_range\": \"0-82 total\",\n        \"direction\": \"higher worse\",\n        \"cutoffs\": {\"anxiety_disorder\": 25},\n        \"subscales\": {\"panic\": 7, \"social\": 8, \"school_phobia\": 3, \"separation\": 5, \"gad\": 9}\n    },\n    \"rses\": {\n        \"scale_range\": \"0-30\", \n        \"direction\": \"varies\",\n        \"cutoffs\": {\"low\": 15, \"normal_min\": 15, \"normal_max\": 25, \"high\": 25}\n    },\n    \"sdq\": {\n        \"scale_range\": \"0-40 total difficulties\",\n        \"direction\": \"higher worse\", \n        \"cutoffs\": {\"normal\": 13, \"borderline\": 16, \"abnormal\": 17}\n    },\n    \"psc-17\": {\n        \"scale_range\": \"0-34\",\n        \"direction\": \"higher worse\",\n        \"cutoffs\": {\"positive_screen\": 15},\n        \"subscales\": {\"internalizing\": 5, \"attention\": 7, \"externalizing\": 7}\n    }\n}\n\ndef to_iso_date(value: Any) -> str:\n    \"\"\"Convert various date formats to ISO date string\"\"\"\n    if not value:\n        return ''\n    try:\n        if isinstance(value, str):\n            # Handle various date formats\n            dt = datetime.fromisoformat(value.replace('Z', '+00:00'))\n        elif hasattr(value, 'year'):\n            # Handle datetime/Timestamp objects (pandas Timestamp, datetime.datetime)\n            dt = value\n        elif isinstance(value, (int, float)):\n            # Handle Excel serial date numbers (days since 1900-01-01)\n            # Excel date system: 1 = 1900-01-01, 2 = 1900-01-02, etc.\n            # Note: Excel has a bug treating 1900 as a leap year, but for dates after 1900-02-28 this doesn't matter\n            excel_epoch = datetime(1899, 12, 30)  # Use Dec 30, 1899 to account for Excel's quirk\n            dt = excel_epoch + timedelta(days=int(value))\n        else:\n            return ''  # Can't parse, return empty instead of today's date\n        return dt.strftime('%Y-%m-%d')\n    except:\n        return ''\n\ndef safe_number(value: Any) -> float:\n    \"\"\"Safely convert value to number\"\"\"\n    try:\n        return float(value) if value is not None else 0.0\n    except:\n        return 0.0\n\ndef safe_round(value: Any) -> int:\n    \"\"\"Safely round value to integer\"\"\"\n    return round(safe_number(value))\n\ndef get_questionnaire_info(questionnaire: str) -> Dict[str, Any]:\n    \"\"\"Get cut-off and scoring information for a questionnaire\"\"\"\n    if not questionnaire:\n        return {}\n    \n    q_name = questionnaire.lower().strip()\n    \n    # Find matching questionnaire in our cut-offs\n    for key, info in QUESTIONNAIRE_CUTOFFS.items():\n        if key in q_name:\n            return info.copy()\n    \n    return {\"scale_range\": \"unknown\", \"direction\": \"unknown\", \"cutoffs\": {}}\n\n# Severity functions based on reference table\ndef phq9_severity(score: int) -> str:\n    \"\"\"PHQ-9 severity: 5=mild, 10=moderate, 15=moderately severe, ≥20=severe\"\"\"\n    if score <= 4:\n        return 'minimal'\n    elif score <= 9:\n        return 'mild'\n    elif score <= 14:\n        return 'moderate'\n    elif score <= 19:\n        return 'moderately severe'\n    else:\n        return 'severe'\n\ndef gad7_severity(score: int) -> str:\n    \"\"\"GAD-7 severity: 5=mild, 10=moderate, 15=severe\"\"\"\n    if score <= 4:\n        return 'minimal'\n    elif score <= 9:\n        return 'mild'\n    elif score <= 14:\n        return 'moderate'\n    else:\n        return 'severe'\n\ndef who5_index(raw_score: int) -> int:\n    \"\"\"WHO-5: raw 0-25 multiplied by 4 = index 0-100\"\"\"\n    return max(0, min(100, raw_score * 4))\n\ndef promis_severity(t_score: float) -> str:\n    \"\"\"PROMIS Pediatric T-score interpretation\"\"\"\n    if t_score <= 55:\n        return 'within normal limits'\n    elif t_score <= 60:\n        return 'mild'\n    elif t_score <= 70:\n        return 'moderate'\n    else:\n        return 'severe'\n\ndef rses_band(score: int) -> str:\n    \"\"\"Rosenberg Self-Esteem Scale bands\"\"\"\n    if score < 15:\n        return 'low'\n    elif score > 25:\n        return 'high'\n    else:\n        return 'normal'\n\ndef detect_sdq_version(questionnaire_name: str) -> str:\n    \"\"\"Detect if SDQ is parent or self-completed version\"\"\"\n    name_lower = questionnaire_name.lower()\n    if 'youth' in name_lower or 'self' in name_lower or 'adolescent' in name_lower:\n        return 'self_completed'\n    elif 'parent' in name_lower or 'teacher' in name_lower:\n        return 'parent'\n    else:\n        # Default to self-completed for youth report (11-17)\n        return 'self_completed'\n\ndef get_sdq_cutoffs(version: str) -> Dict[str, Dict[str, Any]]:\n    \"\"\"Get SDQ cut-offs based on version (parent or self-completed)\"\"\"\n    if version == 'self_completed':\n        return {\n            'total_difficulties': {\n                'normal': (0, 15),\n                'borderline': (16, 19),\n                'abnormal': (20, 40)\n            },\n            'emotional': {\n                'normal': (0, 5),\n                'borderline': (6, 6),\n                'abnormal': (7, 10)\n            },\n            'conduct': {\n                'normal': (0, 3),\n                'borderline': (4, 4),\n                'abnormal': (5, 10)\n            },\n            'hyperactivity': {\n                'normal': (0, 5),\n                'borderline': (6, 6),\n                'abnormal': (7, 10)\n            },\n            'peer_problems': {\n                'normal': (0, 3),\n                'borderline': (4, 5),\n                'abnormal': (6, 10)\n            },\n            'prosocial': {\n                'normal': (6, 10),\n                'borderline': (5, 5),\n                'abnormal': (0, 4)\n            }\n        }\n    else:  # parent version\n        return {\n            'total_difficulties': {\n                'normal': (0, 13),\n                'borderline': (14, 16),\n                'abnormal': (17, 40)\n            },\n            'emotional': {\n                'normal': (0, 3),\n                'borderline': (4, 4),\n                'abnormal': (5, 10)\n            },\n            'conduct': {\n                'normal': (0, 2),\n                'borderline': (3, 3),\n                'abnormal': (4, 10)\n            },\n            'hyperactivity': {\n                'normal': (0, 5),\n                'borderline': (6, 6),\n                'abnormal': (7, 10)\n            },\n            'peer_problems': {\n                'normal': (0, 2),\n                'borderline': (3, 3),\n                'abnormal': (4, 10)\n            },\n            'prosocial': {\n                'normal': (6, 10),\n                'borderline': (5, 5),\n                'abnormal': (0, 4)\n            }\n        }\n\ndef interpret_sdq_score(score: int, subscale: str, cutoffs: Dict[str, tuple]) -> Dict[str, Any]:\n    \"\"\"Interpret a single SDQ score against cut-offs\"\"\"\n    if subscale not in cutoffs:\n        return {'band': 'unknown', 'interpretation': 'No cut-offs available'}\n    \n    ranges = cutoffs[subscale]\n    \n    # Check which band the score falls into\n    if ranges['normal'][0] <= score <= ranges['normal'][1]:\n        band = 'normal'\n        interpretation = 'close to average - clinically significant problems in this area are unlikely'\n    elif ranges['borderline'][0] <= score <= ranges['borderline'][1]:\n        band = 'borderline'\n        if subscale == 'prosocial':\n            interpretation = 'slightly low, which may reflect clinically significant problems'\n        else:\n            interpretation = 'slightly raised, which may reflect clinically significant problems'\n    else:  # abnormal range\n        band = 'abnormal'\n        if subscale == 'prosocial':\n            interpretation = 'low - there is a substantial risk of clinically significant problems in this area'\n        else:\n            interpretation = 'high - there is a substantial risk of clinically significant problems in this area'\n    \n    return {\n        'score': score,\n        'band': band,\n        'interpretation': interpretation\n    }\n\n\ndef normalize_text(text: str) -> str:\n    \"\"\"Normalize text for comparison\"\"\"\n    return str(text or '').strip().lower()\n\ndef includes_any(text: str, keywords: List[str]) -> bool:\n    \"\"\"Check if text includes any of the keywords\"\"\"\n    norm_text = normalize_text(text)\n    return any(normalize_text(keyword) in norm_text for keyword in keywords)\n\ndef score_subscales(responses: List[Dict], mapping: Dict[str, callable]) -> Dict[str, Dict[str, int]]:\n    \"\"\"Score subscales based on dimension mapping\"\"\"\n    subscales = {}\n    for name, predicate in mapping.items():\n        matching_responses = [r for r in responses if predicate(r)]\n        total = sum(safe_number(r.get('answer', 0)) for r in matching_responses)\n        subscales[name] = {\n            'total': int(total),\n            'count': len(matching_responses)\n        }\n    return subscales\n\ndef preprocess_questionnaire_data(items: List[Dict]) -> List[Dict]:\n    \"\"\"\n    Main preprocessing function for questionnaire data\n    \n    Args:\n        items: List of raw questionnaire items from n8n\n        \n    Returns:\n        List of processed items with computed scores, severities, and flags\n    \"\"\"\n    \n    # Start preprocessing (quiet mode for n8n Code node)\n    \n    # Step A: Normalize individual rows\n    rows = []\n    questionnaire_counts = {}\n    \n    for item in items:\n        json_data = item.get('json', {})\n        questionnaire = str(json_data.get('questionnaire', '')).strip()\n        \n        # Count questionnaires for debugging\n        questionnaire_counts[questionnaire] = questionnaire_counts.get(questionnaire, 0) + 1\n        \n        # Skip rows with NaN/None/empty questionnaire (metadata rows)\n        if not questionnaire or questionnaire.lower() in ['nan', 'none', '<na>', 'null']:\n            continue\n        \n        # Try both 'timepoint' (singular) and 'timepoints' (plural) for flexibility\n        timepoint_value = json_data.get('timepoint', json_data.get('timepoints', 0))\n        date_str = to_iso_date(json_data.get('date'))\n        dim_str = str(json_data.get('dimension', '')).strip()\n\n        # Note: We no longer skip rows that are missing timepoint/date/dimension; they will be included as-is\n            \n        row = {\n            'questionnaire': questionnaire,\n            'timepoint': safe_round(timepoint_value),\n            'date': date_str,\n            'question': str(json_data.get('question', '')).strip(),\n            'answer_int': safe_number(json_data.get('answer', 0)),\n            'answer_raw': safe_number(json_data.get('answer', 0)),\n            'dimension': dim_str,\n            'free_text': str(json_data.get('free_text', '')).strip() if json_data.get('free_text') and not (isinstance(json_data.get('free_text'), float) and math.isnan(json_data.get('free_text'))) else '',\n            'response_options': str(json_data.get('response_options', '')).strip()\n        }\n        rows.append(row)\n    \n    # Questionnaire counts collected (not printed in n8n)\n    \n    # Step B: Group by questionnaire + timepoint\n    groups = {}\n    for row in rows:\n        key = f\"{row['questionnaire']}::{row['timepoint']}\"\n        if key not in groups:\n            groups[key] = {\n                'questionnaire': row['questionnaire'],\n                'timepoint': row['timepoint'],\n                'date': row['date'],\n                'responses': [],\n                'free_text': row['free_text']\n            }\n        \n        # Accumulate free text if present in this row\n        if row['free_text'] and row['free_text'] not in groups[key]['free_text']:\n            if groups[key]['free_text']:\n                groups[key]['free_text'] += ' | ' + row['free_text']\n            else:\n                groups[key]['free_text'] = row['free_text']\n        \n        groups[key]['responses'].append({\n            'question': row['question'],\n            'answer': row['answer_int'],\n            'dimension': row['dimension'],\n            'response_options': row['response_options']\n        })\n    \n    # Groups created (quiet)\n    \n    # Step C: Process each questionnaire group with cut-off focus\n    results = []\n    for group in groups.values():\n        name = normalize_text(group['questionnaire'])\n        total = sum(safe_number(r.get('answer', 0)) for r in group['responses'])\n        \n        # Get questionnaire-specific cut-off information\n        q_info = get_questionnaire_info(group['questionnaire'])\n        \n        result = {\n            'questionnaire': group['questionnaire'],\n            'timepoint': group['timepoint'],\n            'date': group['date'],\n            'raw_total': int(total),\n            'scale_info': {\n                'range': q_info.get('scale_range', 'unknown'),\n                'direction': q_info.get('direction', 'unknown'),\n                'cutoffs': q_info.get('cutoffs', {})\n            },\n            'severity': '',\n            'clinical_flags': [],\n            'derived': {},  # Initialize derived dictionary\n            'responses': group['responses'],\n            'free_text': group['free_text']\n        }\n        \n        # PHQ-9\n        if 'phq-9' in name or 'phq9' in name or 'phq' in name:\n            cutoffs = q_info.get('cutoffs', {})\n            result['severity'] = phq9_severity(int(total))\n            result['derived']['scale'] = 'PHQ-9 (0-27, higher worse)'\n            result['derived']['severity_level'] = result['severity']\n            result['derived']['total_score'] = int(total)\n            \n            # Apply clinical cut-offs\n            if total >= cutoffs.get('severe', 20):\n                result['clinical_flags'].append(f'PHQ-9 ≥{cutoffs.get(\"severe\", 20)} (severe depression)')\n            elif total >= cutoffs.get('moderately_severe', 15):\n                result['clinical_flags'].append(f'PHQ-9 ≥{cutoffs.get(\"moderately_severe\", 15)} (moderately severe)')\n            elif total >= cutoffs.get('moderate', 10):\n                result['clinical_flags'].append(f'PHQ-9 ≥{cutoffs.get(\"moderate\", 10)} (moderate depression)')\n            elif total >= cutoffs.get('mild', 5):\n                result['clinical_flags'].append(f'PHQ-9 ≥{cutoffs.get(\"mild\", 5)} (mild depression)')\n                \n            # Clinical significance flag\n            clinical_flag = q_info.get('clinical_flag', {})\n            if total >= clinical_flag.get('threshold', 10):\n                result['clinical_flags'].append(f'PHQ-9 ≥{clinical_flag.get(\"threshold\", 10)} suggests {clinical_flag.get(\"meaning\", \"clinical attention\")}')\n        \n        # WHO-5\n        elif any(x in name for x in ['who-5', 'who5', 'who 5']):\n            cutoffs = q_info.get('cutoffs', {})\n            index = who5_index(int(total))\n            result['who5_index'] = index\n            result['derived']['scale'] = 'WHO-5 (0-100 index, lower worse)'\n            result['derived']['raw_score'] = int(total)\n            result['derived']['total_score'] = int(total)  # Add total_score for consistency\n            result['derived']['index_score'] = index\n            result['severity'] = 'reduced well-being' if index <= cutoffs.get('poor_wellbeing', 50) else 'adequate well-being'\n            result['derived']['severity_level'] = result['severity']\n            \n            # Apply WHO-5 cut-offs\n            if index <= cutoffs.get('depression_risk', 28):\n                result['clinical_flags'].append(f'WHO-5 ≤{cutoffs.get(\"depression_risk\", 28)} indicates depression risk')\n            elif index <= cutoffs.get('poor_wellbeing', 50):\n                result['clinical_flags'].append(f'WHO-5 ≤{cutoffs.get(\"poor_wellbeing\", 50)} suggests poor well-being')\n        \n        # GAD-7\n        elif any(x in name for x in ['gad-7', 'gad7', 'gad 7']):\n            cutoffs = q_info.get('cutoffs', {})\n            result['severity'] = gad7_severity(int(total))\n            result['derived']['scale'] = 'GAD-7 (0-21, higher worse)'\n            result['derived']['severity_level'] = result['severity']\n            result['derived']['total_score'] = int(total)\n            \n            # Apply GAD-7 cut-offs\n            if total >= cutoffs.get('severe', 15):\n                result['clinical_flags'].append(f'GAD-7 ≥{cutoffs.get(\"severe\", 15)} (severe anxiety)')\n            elif total >= cutoffs.get('moderate', 10):\n                result['clinical_flags'].append(f'GAD-7 ≥{cutoffs.get(\"moderate\", 10)} (moderate anxiety)')\n            elif total >= cutoffs.get('mild', 5):\n                result['clinical_flags'].append(f'GAD-7 ≥{cutoffs.get(\"mild\", 5)} (mild anxiety)')\n        \n        # PROMIS (Depression, Anxiety, Life Satisfaction)\n        elif 'promis' in name:\n            # T-score conversion tables\n            PROMIS_DEPRESSION_PEDIATRIC = {\n                8: 39.9, 9: 46.9, 10: 49.3, 11: 51.0, 12: 52.4, 13: 53.6, 14: 54.6, 15: 55.6,\n                16: 56.5, 17: 57.4, 18: 58.3, 19: 59.1, 20: 60.0, 21: 60.8, 22: 61.7, 23: 62.5,\n                24: 63.3, 25: 64.1, 26: 64.9, 27: 65.7, 28: 66.5, 29: 67.3, 30: 68.0, 31: 68.8,\n                32: 69.6, 33: 70.4, 34: 71.2, 35: 72.1, 36: 73.1, 37: 74.2, 38: 75.5, 39: 77.2, 40: 80.3\n            }\n            \n            PROMIS_ANXIETY_PEDIATRIC = {\n                8: 39.0, 9: 45.4, 10: 47.8, 11: 49.6, 12: 51.0, 13: 52.2, 14: 53.3, 15: 54.4,\n                16: 55.3, 17: 56.3, 18: 57.2, 19: 58.1, 20: 59.0, 21: 59.9, 22: 60.8, 23: 61.7,\n                24: 62.6, 25: 63.4, 26: 64.3, 27: 65.1, 28: 65.9, 29: 66.8, 30: 67.6, 31: 68.4,\n                32: 69.2, 33: 70.0, 34: 70.9, 35: 71.8, 36: 72.8, 37: 73.9, 38: 75.2, 39: 76.7, 40: 79.8\n            }\n            \n            PROMIS_LIFE_SATISFACTION_PEDIATRIC = {\n                8: 20.5, 9: 23.6, 10: 25.3, 11: 26.7, 12: 27.9, 13: 28.9, 14: 29.9, 15: 30.7,\n                16: 31.6, 17: 32.5, 18: 33.3, 19: 34.1, 20: 34.9, 21: 35.8, 22: 36.6, 23: 37.4,\n                24: 38.3, 25: 39.1, 26: 40.0, 27: 40.9, 28: 41.9, 29: 42.9, 30: 43.9, 31: 44.9,\n                32: 45.9, 33: 46.9, 34: 48.1, 35: 49.2, 36: 50.5, 37: 52.0, 38: 53.9, 39: 56.7, 40: 62.5\n            }\n            \n            PROMIS_DEPRESSION_PARENT = {\n                6: 40.8, 7: 48.2, 8: 51.1, 9: 53.2, 10: 54.9, 11: 56.4, 12: 57.9, 13: 59.2,\n                14: 60.6, 15: 61.9, 16: 63.2, 17: 64.6, 18: 65.9, 19: 67.1, 20: 68.3, 21: 69.6,\n                22: 70.7, 23: 71.9, 24: 73.0, 25: 74.2, 26: 75.4, 27: 76.7, 28: 78.2, 29: 79.8, 30: 82.7\n            }\n            \n            PROMIS_ANXIETY_PARENT = {\n                8: 38.8, 9: 45.2, 10: 48.0, 11: 49.9, 12: 51.5, 13: 52.8, 14: 54.0, 15: 55.2,\n                16: 56.3, 17: 57.3, 18: 58.4, 19: 59.4, 20: 60.4, 21: 61.4, 22: 62.5, 23: 63.4,\n                24: 64.4, 25: 65.3, 26: 66.3, 27: 67.2, 28: 68.1, 29: 69.0, 30: 69.9, 31: 70.8,\n                32: 71.7, 33: 72.6, 34: 73.5, 35: 74.5, 36: 75.6, 37: 76.8, 38: 78.2, 39: 80.0, 40: 82.7\n            }\n            \n            PROMIS_LIFE_SATISFACTION_PARENT = {\n                8: 18.5, 9: 21.4, 10: 22.9, 11: 24.1, 12: 25.2, 13: 26.1, 14: 27.0, 15: 27.8,\n                16: 28.6, 17: 29.4, 18: 30.2, 19: 31.0, 20: 31.8, 21: 32.7, 22: 33.5, 23: 34.4,\n                24: 35.3, 25: 36.2, 26: 37.2, 27: 38.2, 28: 39.2, 29: 40.3, 30: 41.5, 31: 42.7,\n                32: 43.9, 33: 45.1, 34: 46.4, 35: 47.7, 36: 49.1, 37: 50.6, 38: 52.5, 39: 55.2, 40: 61.5\n            }\n            \n            def get_promis_t_score(raw_total, conversion_table):\n                \"\"\"Convert raw PROMIS score to T-score using lookup table\"\"\"\n                return conversion_table.get(raw_total, None)\n            \n            def interpret_promis_t_score(t_score, measure_type):\n                \"\"\"Interpret PROMIS T-score based on measure type\"\"\"\n                if measure_type in ['depression', 'anxiety']:\n                    # Higher scores = worse (negative measures)\n                    if t_score <= 50:\n                        return {\n                            'severity': 'within normal limits',\n                            'interpretation': 'Within Normal Limits'\n                        }\n                    elif t_score <= 55:\n                        return {\n                            'severity': 'mild',\n                            'interpretation': 'Mild'\n                        }\n                    elif t_score <= 65:\n                        return {\n                            'severity': 'moderate',\n                            'interpretation': 'Moderate'\n                        }\n                    else:\n                        return {\n                            'severity': 'severe',\n                            'interpretation': 'Severe'\n                        }\n                else:  # life satisfaction\n                    # Higher scores = better (positive measure)\n                    if t_score >= 70:\n                        return {\n                            'severity': 'very high',\n                            'interpretation': 'Very High'\n                        }\n                    elif t_score >= 60:\n                        return {\n                            'severity': 'high',\n                            'interpretation': 'High'\n                        }\n                    elif t_score >= 40:\n                        return {\n                            'severity': 'average',\n                            'interpretation': 'Average'\n                        }\n                    elif t_score >= 30:\n                        return {\n                            'severity': 'low',\n                            'interpretation': 'Low'\n                        }\n                    else:\n                        return {\n                            'severity': 'very low',\n                            'interpretation': 'Very Low'\n                        }\n            \n            # Determine measure type and version\n            is_parent = 'parent' in name.lower()\n            measure_type = None\n            conversion_table = None\n            \n            if 'depression' in name:\n                measure_type = 'depression'\n                conversion_table = PROMIS_DEPRESSION_PARENT if is_parent else PROMIS_DEPRESSION_PEDIATRIC\n                result['derived']['scale'] = f'PROMIS Depression {\"Parent Proxy\" if is_parent else \"Pediatric\"} T-score (mean 50, SD 10, higher worse)'\n                result['derived']['note'] = 'Higher T-scores indicate more depression symptoms'\n            elif 'anxiety' in name:\n                measure_type = 'anxiety'\n                conversion_table = PROMIS_ANXIETY_PARENT if is_parent else PROMIS_ANXIETY_PEDIATRIC\n                result['derived']['scale'] = f'PROMIS Anxiety {\"Parent Proxy\" if is_parent else \"Pediatric\"} T-score (mean 50, SD 10, higher worse)'\n                result['derived']['note'] = 'Higher T-scores indicate more anxiety symptoms'\n            elif 'life' in name or 'satisfaction' in name:\n                measure_type = 'life_satisfaction'\n                conversion_table = PROMIS_LIFE_SATISFACTION_PARENT if is_parent else PROMIS_LIFE_SATISFACTION_PEDIATRIC\n                result['derived']['scale'] = f'PROMIS Life Satisfaction {\"Parent Proxy\" if is_parent else \"Pediatric\"} T-score (mean 50, SD 10, higher better)'\n                result['derived']['note'] = 'Higher T-scores indicate better life satisfaction'\n            else:\n                result['derived']['scale'] = 'PROMIS Pediatric T-score (mean 50, SD 10)'\n                result['derived']['note'] = 'Unknown PROMIS measure - cannot convert to T-score'\n            \n            # Store raw scores\n            result['derived']['raw_score'] = int(total)\n            result['derived']['total_score'] = int(total)\n            \n            # Convert to T-score if table available\n            if conversion_table and measure_type:\n                t_score = get_promis_t_score(int(total), conversion_table)\n                \n                if t_score is not None:\n                    result['derived']['t_score'] = round(t_score, 1)\n                    \n                    # Get interpretation\n                    interpretation = interpret_promis_t_score(t_score, measure_type)\n                    result['severity'] = interpretation['severity']\n                    result['derived']['severity_level'] = result['severity']\n                    result['derived']['interpretation'] = interpretation['interpretation']\n                    \n                    # Add clinical flags based on T-score thresholds\n                    if measure_type in ['depression', 'anxiety']:\n                        if t_score > 65:\n                            result['clinical_flags'].append(f'PROMIS {measure_type.title()} T-score {t_score:.1f} (Severe - significant clinical concern)')\n                        elif t_score > 55:\n                            result['clinical_flags'].append(f'PROMIS {measure_type.title()} T-score {t_score:.1f} (Moderate - clinical attention warranted)')\n                        elif t_score > 50:\n                            result['clinical_flags'].append(f'PROMIS {measure_type.title()} T-score {t_score:.1f} (Mild - monitor)')\n                    else:  # life satisfaction\n                        if t_score < 30:\n                            result['clinical_flags'].append(f'PROMIS Life Satisfaction T-score {t_score:.1f} (Very Low - significant concern)')\n                        elif t_score < 40:\n                            result['clinical_flags'].append(f'PROMIS Life Satisfaction T-score {t_score:.1f} (Low - below average)')\n                    \n                    result['clinical_flags'].append(f'PROMIS {measure_type.replace(\"_\", \" \").title()}: Raw={int(total)}, T-score={t_score:.1f} ({interpretation[\"interpretation\"]})')\n                \n                else:\n                    # Raw score outside conversion table range\n                    result['severity'] = 'raw score outside conversion range'\n                    result['derived']['severity_level'] = result['severity']\n                    result['clinical_flags'].append(f'PROMIS raw total {int(total)} outside conversion table range (8-40)')\n            \n            else:\n                # No conversion table available\n                result['severity'] = 'unknown PROMIS measure'\n                result['derived']['severity_level'] = result['severity']\n                result['clinical_flags'].append(f'PROMIS raw total: {int(total)}. Unable to convert - unknown measure type.')\n        \n        # PedsQL - Calculate both Total Score and Psychosocial Score for ratio-based interpretation\n        elif 'pedsql' in name:\n            result['derived']['scale'] = 'PedsQL Psychosocial/Total Score (0-100, higher better)'\n            result['derived']['note'] = 'Scores reverse-transformed: 0→100, 1→75, 2→50, 3→25, 4→0. Interpretation based on Psychosocial/Total Score ratio'\n            \n            # Transform raw scores (0-4) to PedsQL scale (0-100)\n            def transform_pedsql_score(raw_score):\n                \"\"\"Transform raw PedsQL score (0-4) to 0-100 scale\"\"\"\n                transformation_map = {0: 100, 1: 75, 2: 50, 3: 25, 4: 0}\n                return transformation_map.get(int(raw_score), None)\n            \n            # Extract question number from question text\n            def get_question_number(question_text):\n                \"\"\"Extract question number from question text (e.g., '1. Question text' -> 1)\"\"\"\n                match = re.match(r'^(\\d+)', str(question_text).strip())\n                if match:\n                    return int(match.group(1))\n                return None\n            \n            # Group responses by ALL dimensions (Physical + Psychosocial)\n            all_dimensions = {\n                'Physical': [],\n                'Emotional': [],\n                'Social': [],\n                'School': []\n            }\n            \n            # Categorize responses by question number\n            # Questions 1-8: Physical, 9-13: Emotional, 14-18: Social, 19-23: School\n            for response in group['responses']:\n                raw_score = response.get('answer', 0)\n                question_text = response.get('question', '')\n                transformed_score = transform_pedsql_score(raw_score)\n                \n                if transformed_score is not None:  # Valid score (0-4 range)\n                    question_num = get_question_number(question_text)\n                    \n                    if question_num is not None:\n                        if 1 <= question_num <= 8:\n                            all_dimensions['Physical'].append(transformed_score)\n                        elif 9 <= question_num <= 13:\n                            all_dimensions['Emotional'].append(transformed_score)\n                        elif 14 <= question_num <= 18:\n                            all_dimensions['Social'].append(transformed_score)\n                        elif 19 <= question_num <= 23:\n                            all_dimensions['School'].append(transformed_score)\n            \n            # Define expected items per dimension\n            PEDSQL_DIMENSION_ITEMS = {\n                'Physical': 8,      # Physical Functioning (8 items)\n                'Emotional': 5,     # Emotional Functioning (5 items) \n                'Social': 5,        # Social Functioning (5 items)\n                'School': 5         # School Functioning (5 items)\n            }\n            \n            # Calculate dimension scores\n            dimension_scores = {}\n            all_total_scores = []\n            all_psychosocial_scores = []\n            \n            for dimension_name, scores in all_dimensions.items():\n                expected_items = PEDSQL_DIMENSION_ITEMS.get(dimension_name, len(scores))\n                answered_items = len(scores)\n                \n                if answered_items >= (expected_items * 0.5):  # At least 50% answered\n                    dimension_mean = sum(scores) / len(scores)\n                    dimension_scores[dimension_name] = {\n                        'score': round(dimension_mean, 2),\n                        'items_answered': answered_items,\n                        'items_expected': expected_items,\n                        'completion_rate': round((answered_items / expected_items) * 100, 1)\n                    }\n                    # Add all individual scores to total pool\n                    all_total_scores.extend(scores)\n                    \n                    # Add psychosocial scores (exclude Physical)\n                    if dimension_name in ['Emotional', 'Social', 'School']:\n                        all_psychosocial_scores.extend(scores)\n                else:\n                    # Don't calculate score - insufficient data\n                    dimension_scores[dimension_name] = {\n                        'score': None,\n                        'items_answered': answered_items,\n                        'items_expected': expected_items,\n                        'completion_rate': round((answered_items / expected_items) * 100, 1),\n                        'reason': 'Insufficient data (>50% missing)'\n                    }\n            \n            # Store detailed results\n            result['derived']['dimension_scores'] = dimension_scores\n            result['derived']['raw_total'] = int(total)  # Keep original sum for reference\n            \n            # Calculate Total Score and Psychosocial Score\n            if all_total_scores and all_psychosocial_scores:\n                total_score = sum(all_total_scores) / len(all_total_scores)\n                psychosocial_score = sum(all_psychosocial_scores) / len(all_psychosocial_scores)\n                \n                # Calculate Psychosocial/Total Score ratio (psychosocial score divided by total score as percentage)\n                psychosocial_total_ratio = (psychosocial_score / total_score) * 100 if total_score > 0 else 0\n                \n                result['derived']['total_score'] = round(total_score, 2)\n                result['derived']['psychosocial_score'] = round(psychosocial_score, 2)\n                result['derived']['psychosocial_total_ratio'] = round(psychosocial_total_ratio, 2)\n                \n                # PedsQL interpretation function based on reference table\n                def get_pedsql_interpretation(score):\n                    \"\"\"Get PedsQL interpretation based on reference table for Psychosocial/Total Score\"\"\"\n                    if score >= 80:\n                        return {\n                            'severity': 'typical range',\n                            'interpretation': 'Typical range',\n                            'mental_health_status': 'Normal wellbeing'\n                        }\n                    elif score >= 70:\n                        return {\n                            'severity': 'slightly below norms',\n                            'interpretation': 'Slightly below norms', \n                            'mental_health_status': 'Mild emotional or adjustment difficulties'\n                        }\n                    elif score >= 60:\n                        return {\n                            'severity': 'noticeably below average',\n                            'interpretation': 'Noticeably below average',\n                            'mental_health_status': 'Possible clinical concern — monitor or screen further'\n                        }\n                    else:  # < 60\n                        return {\n                            'severity': 'significantly impaired',\n                            'interpretation': 'Significantly impaired',\n                            'mental_health_status': 'Likely emotional/mental-health problems'\n                        }\n                \n                # Apply interpretation to Psychosocial/Total Score ratio\n                ratio_interpretation = get_pedsql_interpretation(psychosocial_total_ratio)\n                result['severity'] = ratio_interpretation['severity']\n                result['derived']['severity_level'] = result['severity']\n                result['derived']['interpretation'] = ratio_interpretation['interpretation']\n                result['derived']['mental_health_status'] = ratio_interpretation['mental_health_status']\n                \n                # Add clinical flags based on Psychosocial/Total Score ratio interpretation\n                if psychosocial_total_ratio < 60:\n                    result['clinical_flags'].append(f'PedsQL Psychosocial/Total Score {psychosocial_total_ratio:.1f} < 60 (significantly impaired - likely emotional/mental-health problems)')\n                elif psychosocial_total_ratio < 70:\n                    result['clinical_flags'].append(f'PedsQL Psychosocial/Total Score {psychosocial_total_ratio:.1f} (60-69: noticeably below average - possible clinical concern)')\n                elif psychosocial_total_ratio < 80:\n                    result['clinical_flags'].append(f'PedsQL Psychosocial/Total Score {psychosocial_total_ratio:.1f} (70-79: slightly below norms - mild emotional/adjustment difficulties)')\n                \n                # Add component scores for reference\n                result['clinical_flags'].append(f'PedsQL Total Score: {total_score:.1f}, Psychosocial Score: {psychosocial_score:.1f}, Ratio: {psychosocial_total_ratio:.1f}%')\n                \n                # Add flags for dimensions with insufficient data\n                for dimension_name, dimension_data in dimension_scores.items():\n                    dimension_score = dimension_data.get('score')\n                    if dimension_score is None:\n                        # Flag dimensions with insufficient data\n                        completion_rate = dimension_data.get('completion_rate', 0)\n                        result['clinical_flags'].append(f'PedsQL {dimension_name}: Insufficient data ({completion_rate}% complete, need ≥50%)')\n            \n            else:\n                result['severity'] = 'insufficient valid responses'\n                result['derived']['severity_level'] = result['severity']\n                result['clinical_flags'].append('PedsQL: No valid responses in 0-4 range for transformation')\n        \n        # CES-DC\n        elif any(x in name for x in ['ces-dc', 'cesdc', 'ces dc']):\n            result['derived']['scale'] = 'CES-DC (≥15 suggests risk for depression)'\n            result['derived']['total_score'] = int(total)\n            result['severity'] = 'depression risk (≥15)' if total >= 15 else 'below risk threshold'\n            result['derived']['severity_level'] = result['severity']\n            if total >= 15:\n                result['clinical_flags'].append('CES-DC positive screen (≥15)')\n        \n        # SCARED\n        elif 'scared' in name:\n            result['derived']['scale'] = 'SCARED (total ≥25 possible anxiety disorder; subscale cut-offs apply)'\n            result['derived']['total_score'] = int(total)\n            result['severity'] = 'possible anxiety disorder (≥25)' if total >= 25 else 'below screening threshold'\n            result['derived']['severity_level'] = result['severity']\n            \n            # Subscales by dimension\n            subscales = score_subscales(group['responses'], {\n                'Panic': lambda r: includes_any(r['dimension'], ['panic']),\n                'Generalized Anxiety (GAD)': lambda r: includes_any(r['dimension'], ['gad', 'generalized']),\n                'Separation': lambda r: includes_any(r['dimension'], ['separation']),\n                'Social': lambda r: includes_any(r['dimension'], ['social']),\n                'School Phobia': lambda r: includes_any(r['dimension'], ['school'])\n            })\n            result['derived']['subscales'] = subscales\n            \n            # Subscale flags\n            if subscales.get('Panic', {}).get('total', 0) >= 7:\n                result['clinical_flags'].append('SCARED Panic ≥7')\n            if subscales.get('Social', {}).get('total', 0) >= 8:\n                result['clinical_flags'].append('SCARED Social ≥8')\n            if subscales.get('School Phobia', {}).get('total', 0) >= 3:\n                result['clinical_flags'].append('SCARED School ≥3')\n            if subscales.get('Separation', {}).get('total', 0) >= 5:\n                result['clinical_flags'].append('SCARED Separation ≥5')\n            if subscales.get('Generalized Anxiety (GAD)', {}).get('total', 0) >= 9:\n                result['clinical_flags'].append('SCARED GAD ≥9')\n        \n        # RSES\n        elif any(x in name for x in ['rosenberg', 'rses']):\n            result['derived']['scale'] = 'RSES (0-30; <15 low, 15-25 normal, >25 high)'\n            result['derived']['note'] = 'Contains reverse-scored items; verify scoring before interpretation'\n            result['derived']['total_score'] = int(total)\n            result['severity'] = rses_band(int(total))\n            result['derived']['severity_level'] = result['severity']\n        \n        # SDQ - Enhanced with version-specific interpretation\n        elif 'sdq' in name:\n            # Detect version (parent or self-completed)\n            sdq_version = detect_sdq_version(group['questionnaire'])\n            sdq_cutoffs = get_sdq_cutoffs(sdq_version)\n            \n            # Subscales by dimension\n            subscales = score_subscales(group['responses'], {\n                'Emotional': lambda r: includes_any(r['dimension'], ['emotional']),\n                'Conduct': lambda r: includes_any(r['dimension'], ['conduct']),\n                'Hyperactivity/Inattention': lambda r: includes_any(r['dimension'], ['hyperactivity', 'inattention']),\n                'Peer Problems': lambda r: includes_any(r['dimension'], ['peer']),\n                'Prosocial': lambda r: includes_any(r['dimension'], ['prosocial'])\n            })\n            \n            # Total difficulties (exclude Prosocial)\n            total_difficulties = (\n                subscales.get('Emotional', {}).get('total', 0) +\n                subscales.get('Conduct', {}).get('total', 0) +\n                subscales.get('Hyperactivity/Inattention', {}).get('total', 0) +\n                subscales.get('Peer Problems', {}).get('total', 0)\n            )\n            \n            # Store raw scores\n            result['derived']['raw_scores'] = {\n                'total_difficulties': total_difficulties,\n                'emotional': subscales.get('Emotional', {}).get('total', 0),\n                'conduct': subscales.get('Conduct', {}).get('total', 0),\n                'hyperactivity': subscales.get('Hyperactivity/Inattention', {}).get('total', 0),\n                'peer_problems': subscales.get('Peer Problems', {}).get('total', 0),\n                'prosocial': subscales.get('Prosocial', {}).get('total', 0)\n            }\n            \n            # Store subscale details for reference\n            result['derived']['subscales'] = subscales\n            \n            # Interpret all scores using version-specific cut-offs\n            result['derived']['interpretations'] = {\n                'version': sdq_version,\n                'total_difficulties': interpret_sdq_score(total_difficulties, 'total_difficulties', sdq_cutoffs),\n                'emotional': interpret_sdq_score(result['derived']['raw_scores']['emotional'], 'emotional', sdq_cutoffs),\n                'conduct': interpret_sdq_score(result['derived']['raw_scores']['conduct'], 'conduct', sdq_cutoffs),\n                'hyperactivity': interpret_sdq_score(result['derived']['raw_scores']['hyperactivity'], 'hyperactivity', sdq_cutoffs),\n                'peer_problems': interpret_sdq_score(result['derived']['raw_scores']['peer_problems'], 'peer_problems', sdq_cutoffs),\n                'prosocial': interpret_sdq_score(result['derived']['raw_scores']['prosocial'], 'prosocial', sdq_cutoffs)\n            }\n            \n            # Set overall severity based on total difficulties\n            total_diff_interpretation = result['derived']['interpretations']['total_difficulties']\n            result['severity'] = total_diff_interpretation['band']\n            \n            # Add scale information with version-specific cut-offs\n            if sdq_version == 'self_completed':\n                result['derived']['scale'] = 'SDQ Total Difficulties - Self-Completed (0-15 normal, 16-19 borderline, 20-40 abnormal)'\n            else:\n                result['derived']['scale'] = 'SDQ Total Difficulties - Parent/Teacher (0-13 normal, 14-16 borderline, 17-40 abnormal)'\n            \n            # Add clinical flags for abnormal subscales\n            for subscale_name, interpretation in result['derived']['interpretations'].items():\n                if subscale_name != 'version' and interpretation.get('band') == 'abnormal':\n                    result['clinical_flags'].append(\n                        f\"SDQ {subscale_name.replace('_', ' ').title()}: {interpretation['score']} - {interpretation['interpretation']}\"\n                    )\n        \n        # PSC-17\n        elif any(x in name for x in ['psc-17', 'psc17', 'psc 17', 'Pediatric Symptom Checklist – 17 (PSC-17)', 'psc']):\n            # PSC-17 processing (quiet)\n            result['derived']['scale'] = 'PSC-17 (total ≥15 positive; subscales Internalizing ≥5, Attention ≥7, Externalizing ≥7)'\n            result['derived']['total_score'] = int(total)\n            result['severity'] = 'positive screen (≥15)' if total >= 15 else 'below threshold'\n            result['derived']['severity_level'] = result['severity']\n            \n            # Subscales by dimension\n            subscales = score_subscales(group['responses'], {\n                'Internalizing': lambda r: includes_any(r['dimension'], ['internalizing']),\n                'Attention': lambda r: includes_any(r['dimension'], ['attention']),\n                'Externalizing': lambda r: includes_any(r['dimension'], ['externalizing'])\n            })\n            result['derived']['subscales'] = subscales\n            \n            # Subscale flags\n            if subscales.get('Internalizing', {}).get('total', 0) >= 5:\n                result['clinical_flags'].append('PSC-17 Internalizing ≥5')\n            if subscales.get('Attention', {}).get('total', 0) >= 7:\n                result['clinical_flags'].append('PSC-17 Attention ≥7')\n            if subscales.get('Externalizing', {}).get('total', 0) >= 7:\n                result['clinical_flags'].append('PSC-17 Externalizing ≥7')\n        \n        # All other questionnaires - use generic cut-off approach\n        else:\n            # Generic processing (quiet)\n            cutoffs = q_info.get('cutoffs', {})\n            result['severity'] = 'see cut-offs for interpretation'\n            result['derived']['scale'] = f'{group[\"questionnaire\"]} ({q_info.get(\"scale_range\", \"unknown range\")})'\n            result['derived']['total_score'] = int(total)\n            result['derived']['direction'] = q_info.get('direction', 'unknown')\n            \n            # Apply any available cut-offs generically\n            for threshold_name, threshold_value in cutoffs.items():\n                if isinstance(threshold_value, (int, float)):\n                    direction = q_info.get('direction', 'higher worse')\n                    if 'higher' in direction and total >= threshold_value:\n                        result['clinical_flags'].append(f'{group[\"questionnaire\"]} ≥{threshold_value} ({threshold_name.replace(\"_\", \" \")})')\n                    elif 'lower' in direction and total <= threshold_value:\n                        result['clinical_flags'].append(f'{group[\"questionnaire\"]} ≤{threshold_value} ({threshold_name.replace(\"_\", \" \")})')\n            \n            # Handle subscales if available\n            subscales_info = q_info.get('subscales', {})\n            if subscales_info:\n                result['derived']['subscale_cutoffs'] = subscales_info\n        \n        results.append({'json': result})\n    \n    return results\n\n# =============================================================================\n# n8n CODE NODE EXECUTION (Direct execution - no function wrappers)\n# =============================================================================\n# Note: In n8n, 'items' is a global variable provided by the platform\n# The following code is designed to run directly in an n8n Code node\n\n# Check if running in n8n environment\nif 'items' in globals():\n    try:\n        processed_items = preprocess_questionnaire_data(items)\n        return processed_items\n    except Exception as e:\n        import traceback\n        error_details = {\n            'error_message': str(e),\n            'error_type': type(e).__name__,\n            'input_items_count': len(items) if 'items' in globals() else 0,\n            'traceback': traceback.format_exc(),\n            'debug_info': {\n                'items_available': 'items' in globals(),\n                'items_type': type(items).__name__ if 'items' in globals() else 'undefined',\n                'help': 'Ensure this node receives a list of items with a json payload per row.'\n            }\n        }\n        return [{'json': error_details}]\n\n"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        624,
        0
      ],
      "id": "6c6dd5b9-9a14-479e-a1b7-c8ebe153329e",
      "name": "data preprocess"
    },
    {
      "parameters": {
        "language": "python",
        "pythonCode": "#!/usr/bin/env python3\n\"\"\"\nn8n Data Quality Validator & Checkpoint\nValidates preprocessed questionnaire data before trend analysis or LLM processing\nUse this as a checkpoint node between Preprocessor and Trend Analyzer\n\"\"\"\nimport json\nfrom datetime import datetime\nfrom typing import Dict, List, Any, Optional\n\ndef parse_date(date_str: str) -> Optional[datetime]:\n    \"\"\"Parse date string to datetime object\"\"\"\n    if not date_str:\n        return None\n    try:\n        return datetime.fromisoformat(date_str.replace('Z', '+00:00'))\n    except:\n        try:\n            return datetime.strptime(date_str, '%Y-%m-%d')\n        except:\n            return None\n\ndef validate_preprocessed_data(items: List[Dict]) -> Dict[str, Any]:\n    \"\"\"\n    Comprehensive validation of preprocessed questionnaire data\n    \n    Args:\n        items: List of preprocessed items from the questionnaire preprocessor\n        \n    Returns:\n        Validation report with data quality metrics, warnings, and pass/fail status\n    \"\"\"\n    \n    validation_report = {\n        \"status\": \"PASS\",  # Will change to FAIL or WARNING if issues found\n        \"total_items\": len(items),\n        \"validation_checks\": {},\n        \"data_quality_metrics\": {},\n        \"warnings\": [],\n        \"errors\": [],\n        \"recommendations\": []\n    }\n    \n    if not items:\n        validation_report[\"status\"] = \"FAIL\"\n        validation_report[\"errors\"].append(\"No data received - empty input\")\n        return validation_report\n    \n    # Extract json data from items\n    data_items = []\n    print(f\"🔍 DEBUG: Received {len(items)} items from preprocessor\")\n    \n    for idx, item in enumerate(items):\n        json_data = item.get('json', {})\n        if json_data:\n            data_items.append(json_data)\n            # Debug first few items\n            if idx < 2:\n                print(f\"🔍 DEBUG: Item {idx} keys: {list(json_data.keys())}\")\n                print(f\"🔍 DEBUG: Item {idx} has derived: {bool(json_data.get('derived'))}\")\n                if json_data.get('derived'):\n                    print(f\"🔍 DEBUG: Item {idx} derived keys: {list(json_data['derived'].keys())}\")\n    \n    print(f\"🔍 DEBUG: Extracted {len(data_items)} valid data items\")\n    \n    if not data_items:\n        validation_report[\"status\"] = \"FAIL\"\n        validation_report[\"errors\"].append(\"No valid JSON data found in items\")\n        return validation_report\n    \n    # =========================================================================\n    # CHECK 1: Required Fields Validation\n    # =========================================================================\n    required_fields = ['questionnaire', 'timepoint', 'raw_total', 'severity']\n    optional_but_recommended = ['date', 'clinical_flags', 'derived', 'responses']\n    \n    items_with_all_required = 0\n    items_with_dates = 0\n    items_with_derived = 0\n    items_with_responses = 0\n    missing_fields_by_item = []\n    \n    for idx, item in enumerate(data_items):\n        missing = [field for field in required_fields if field not in item or (item[field] is None or (isinstance(item[field], str) and not item[field].strip()))]\n        if not missing:\n            items_with_all_required += 1\n        else:\n            missing_fields_by_item.append({\n                'item_index': idx,\n                'questionnaire': item.get('questionnaire', 'Unknown'),\n                'missing_fields': missing\n            })\n        \n        if item.get('date'):\n            items_with_dates += 1\n        if item.get('derived') and isinstance(item['derived'], dict) and item['derived']:\n            items_with_derived += 1\n        if item.get('responses'):\n            items_with_responses += 1\n    \n    validation_report[\"validation_checks\"][\"required_fields\"] = {\n        \"pass\": len(missing_fields_by_item) == 0,\n        \"items_with_all_required\": items_with_all_required,\n        \"items_missing_fields\": len(missing_fields_by_item),\n        \"details\": missing_fields_by_item[:5]  # Show first 5 only\n    }\n    \n    if missing_fields_by_item:\n        validation_report[\"status\"] = \"FAIL\"\n        validation_report[\"errors\"].append(\n            f\"{len(missing_fields_by_item)} items missing required fields: {required_fields}\"\n        )\n    \n    # =========================================================================\n    # CHECK 2: Date/Timepoint Quality\n    # =========================================================================\n    valid_dates = []\n    valid_timepoints = []\n    invalid_dates = []\n    \n    for idx, item in enumerate(data_items):\n        # Check dates\n        date_str = item.get('date', '')\n        date_obj = parse_date(date_str)\n        if date_obj:\n            valid_dates.append(date_obj)\n        elif date_str and date_str.strip():\n            invalid_dates.append({\n                'item_index': idx,\n                'questionnaire': item.get('questionnaire', 'Unknown'),\n                'invalid_date': date_str\n            })\n        \n        # Check timepoints\n        timepoint = item.get('timepoint', 0)\n        if timepoint and timepoint > 0:\n            valid_timepoints.append(timepoint)\n    \n    # Determine date range\n    date_range_info = None\n    if valid_dates:\n        valid_dates.sort()\n        date_range_info = {\n            \"earliest\": valid_dates[0].strftime('%Y-%m-%d'),\n            \"latest\": valid_dates[-1].strftime('%Y-%m-%d'),\n            \"span_days\": (valid_dates[-1] - valid_dates[0]).days,\n            \"total_with_dates\": len(valid_dates)\n        }\n    \n    # Determine timepoint range\n    timepoint_range_info = None\n    if valid_timepoints:\n        valid_timepoints_sorted = sorted(valid_timepoints)\n        timepoint_range_info = {\n            \"earliest\": valid_timepoints_sorted[0],\n            \"latest\": valid_timepoints_sorted[-1],\n            \"unique_timepoints\": len(set(valid_timepoints)),\n            \"total_with_timepoints\": len(valid_timepoints)\n        }\n    \n    validation_report[\"validation_checks\"][\"date_timepoint_quality\"] = {\n        \"items_with_dates\": len(valid_dates),\n        \"items_with_valid_timepoints\": len(valid_timepoints),\n        \"invalid_dates\": len(invalid_dates),\n        \"date_range\": date_range_info,\n        \"timepoint_range\": timepoint_range_info\n    }\n    \n    # Warnings for date/timepoint issues\n    if len(valid_dates) < len(data_items):\n        pct = (len(valid_dates) / len(data_items)) * 100\n        validation_report[\"warnings\"].append(\n            f\"Only {len(valid_dates)}/{len(data_items)} ({pct:.1f}%) items have valid dates\"\n        )\n        if pct < 50:\n            validation_report[\"recommendations\"].append(\n                \"Consider adding dates to more items for better trend analysis\"\n            )\n    \n    if invalid_dates:\n        validation_report[\"warnings\"].append(\n            f\"{len(invalid_dates)} items have invalid date formats\"\n        )\n    \n    # =========================================================================\n    # CHECK 3: Questionnaire Distribution & Grouping\n    # =========================================================================\n    questionnaire_groups = {}\n    \n    for item in data_items:\n        q_name = item.get('questionnaire', 'Unknown')\n        if q_name not in questionnaire_groups:\n            questionnaire_groups[q_name] = {\n                'count': 0,\n                'timepoints': set(),\n                'has_dates': 0,\n                'has_derived': 0,\n                'total_scores': []\n            }\n        \n        questionnaire_groups[q_name]['count'] += 1\n        questionnaire_groups[q_name]['timepoints'].add(item.get('timepoint', 0))\n        if item.get('date'):\n            questionnaire_groups[q_name]['has_dates'] += 1\n        if item.get('derived') and item['derived']:\n            questionnaire_groups[q_name]['has_derived'] += 1\n        questionnaire_groups[q_name]['total_scores'].append(item.get('raw_total', 0))\n    \n    # Convert sets to counts for JSON serialization\n    questionnaire_summary = {}\n    questionnaires_ready_for_trends = []\n    questionnaires_insufficient_data = []\n    \n    for q_name, info in questionnaire_groups.items():\n        timepoint_count = len(info['timepoints'])\n        questionnaire_summary[q_name] = {\n            'total_assessments': info['count'],\n            'unique_timepoints': timepoint_count,\n            'assessments_with_dates': info['has_dates'],\n            'assessments_with_derived': info['has_derived'],\n            'score_range': {\n                'min': min(info['total_scores']) if info['total_scores'] else 0,\n                'max': max(info['total_scores']) if info['total_scores'] else 0\n            }\n        }\n        \n        # Check if ready for trend analysis (needs 2+ timepoints)\n        if timepoint_count >= 2:\n            questionnaires_ready_for_trends.append(q_name)\n        else:\n            questionnaires_insufficient_data.append(q_name)\n    \n    validation_report[\"data_quality_metrics\"][\"questionnaire_distribution\"] = {\n        \"total_questionnaires\": len(questionnaire_groups),\n        \"questionnaires_ready_for_trends\": len(questionnaires_ready_for_trends),\n        \"questionnaires_insufficient_data\": len(questionnaires_insufficient_data),\n        \"summary\": questionnaire_summary\n    }\n    \n    # Warnings for questionnaires with insufficient data\n    if questionnaires_insufficient_data:\n        validation_report[\"warnings\"].append(\n            f\"{len(questionnaires_insufficient_data)} questionnaire(s) have only 1 timepoint - cannot analyze trends: {', '.join(questionnaires_insufficient_data)}\"\n        )\n        validation_report[\"recommendations\"].append(\n            \"Collect data at multiple timepoints to enable trend analysis\"\n        )\n    \n    # =========================================================================\n    # CHECK 4: Derived Data Completeness\n    # =========================================================================\n    items_with_empty_derived = []\n    items_with_scale_info = 0\n    items_with_interpretations = 0\n    \n    # Debug info for first few items\n    debug_samples = []\n    \n    for idx, item in enumerate(data_items):\n        derived = item.get('derived')\n        \n        # Collect debug info for first 3 items\n        if idx < 3:\n            debug_info = {\n                'item_index': idx,\n                'questionnaire': item.get('questionnaire', 'Unknown'),\n                'derived_exists': derived is not None,\n                'derived_type': str(type(derived)),\n                'derived_is_dict': isinstance(derived, dict),\n                'has_to_py_method': hasattr(derived, 'to_py') if derived else False,\n                'has_scale': False,\n                'has_interpretations': False,\n                'derived_keys': None\n            }\n            \n            # Try to extract info from derived data\n            if derived:\n                if isinstance(derived, dict):\n                    debug_info['derived_keys'] = list(derived.keys())\n                    debug_info['has_scale'] = bool(derived.get('scale'))\n                    debug_info['has_interpretations'] = bool(derived.get('interpretations'))\n                elif hasattr(derived, 'to_py'):\n                    try:\n                        derived_py = derived.to_py()\n                        if isinstance(derived_py, dict):\n                            debug_info['derived_keys'] = list(derived_py.keys())\n                            debug_info['has_scale'] = bool(derived_py.get('scale'))\n                            debug_info['has_interpretations'] = bool(derived_py.get('interpretations'))\n                    except:\n                        pass\n                elif hasattr(derived, 'scale'):\n                    try:\n                        debug_info['has_scale'] = bool(derived.scale)\n                        debug_info['has_interpretations'] = bool(getattr(derived, 'interpretations', False))\n                    except:\n                        pass\n            \n            debug_samples.append(debug_info)\n        \n        # Handle both Python dicts and JavaScript proxy objects from n8n\n        is_valid_derived = False\n        if derived:\n            if isinstance(derived, dict):\n                # Python dictionary\n                is_valid_derived = True\n                if derived.get('scale'):\n                    items_with_scale_info += 1\n                if derived.get('interpretations'):\n                    items_with_interpretations += 1\n            elif hasattr(derived, 'to_py'):\n                # JavaScript proxy object - convert to Python\n                try:\n                    derived_py = derived.to_py()\n                    if isinstance(derived_py, dict):\n                        is_valid_derived = True\n                        if derived_py.get('scale'):\n                            items_with_scale_info += 1\n                        if derived_py.get('interpretations'):\n                            items_with_interpretations += 1\n                except:\n                    pass\n            elif hasattr(derived, '__getitem__'):\n                # Try to access as object with properties\n                try:\n                    is_valid_derived = True\n                    if hasattr(derived, 'scale') and derived.scale:\n                        items_with_scale_info += 1\n                    if hasattr(derived, 'interpretations') and derived.interpretations:\n                        items_with_interpretations += 1\n                except:\n                    pass\n        \n        if not is_valid_derived:\n            items_with_empty_derived.append({\n                'item_index': idx,\n                'questionnaire': item.get('questionnaire', 'Unknown'),\n                'timepoint': item.get('timepoint', '?')\n            })\n\n    # Calculate derived count based on items that are NOT empty\n    final_items_with_derived = len(data_items) - len(items_with_empty_derived)\n    \n    validation_report[\"validation_checks\"][\"derived_data_quality\"] = {\n        \"items_with_derived\": final_items_with_derived,\n        \"items_with_empty_derived\": len(items_with_empty_derived),\n        \"items_with_scale_info\": items_with_scale_info,\n        \"items_with_interpretations\": items_with_interpretations,\n        \"debug_info\": {\n            \"total_data_items\": len(data_items),\n            \"sample_items\": debug_samples,\n            \"calculation\": f\"{len(data_items)} total - {len(items_with_empty_derived)} empty = {final_items_with_derived} with derived\"\n        }\n    }\n    \n    if items_with_empty_derived:\n        if len(items_with_empty_derived) == len(data_items):\n            validation_report[\"status\"] = \"FAIL\"\n            validation_report[\"errors\"].append(\n                \"All items have empty 'derived' dictionary - preprocessor may not be working correctly\"\n            )\n        else:\n            validation_report[\"warnings\"].append(\n                f\"{len(items_with_empty_derived)} items have empty 'derived' data\"\n            )\n    \n    # =========================================================================\n    # CHECK 5: Score Validity\n    # =========================================================================\n    items_with_zero_scores = []\n    items_with_negative_scores = []\n    \n    for idx, item in enumerate(data_items):\n        raw_total = item.get('raw_total', 0)\n        if raw_total == 0:\n            items_with_zero_scores.append({\n                'item_index': idx,\n                'questionnaire': item.get('questionnaire', 'Unknown'),\n                'timepoint': item.get('timepoint', '?')\n            })\n        elif raw_total < 0:\n            items_with_negative_scores.append({\n                'item_index': idx,\n                'questionnaire': item.get('questionnaire', 'Unknown'),\n                'raw_total': raw_total\n            })\n    \n    validation_report[\"validation_checks\"][\"score_validity\"] = {\n        \"items_with_zero_scores\": len(items_with_zero_scores),\n        \"items_with_negative_scores\": len(items_with_negative_scores)\n    }\n    \n    if items_with_negative_scores:\n        validation_report[\"status\"] = \"FAIL\"\n        validation_report[\"errors\"].append(\n            f\"{len(items_with_negative_scores)} items have negative scores (invalid)\"\n        )\n    \n    if len(items_with_zero_scores) > len(data_items) * 0.5:\n        validation_report[\"warnings\"].append(\n            f\"{len(items_with_zero_scores)} items have zero scores - verify this is expected\"\n        )\n    \n    # =========================================================================\n    # CHECK 6: Trend Analysis Readiness\n    # =========================================================================\n    can_analyze_trends = False\n    sort_method = None\n    trend_readiness_issues = []\n    \n    if len(valid_dates) >= 2:\n        can_analyze_trends = True\n        sort_method = \"date_primary\"\n    elif len(valid_timepoints) >= 2:\n        can_analyze_trends = True\n        sort_method = \"timepoint_only\"\n        trend_readiness_issues.append(\"No dates available - will use timepoint ordering only\")\n    else:\n        trend_readiness_issues.append(\"Need at least 2 items with dates or timepoints for trend analysis\")\n    \n    validation_report[\"data_quality_metrics\"][\"trend_analysis_readiness\"] = {\n        \"ready_for_trend_analysis\": can_analyze_trends,\n        \"recommended_sort_method\": sort_method,\n        \"issues\": trend_readiness_issues,\n        \"questionnaires_with_trends\": len(questionnaires_ready_for_trends),\n        \"total_questionnaires\": len(questionnaire_groups)\n    }\n    \n    if not can_analyze_trends:\n        validation_report[\"warnings\"].append(\n            \"Insufficient data for trend analysis - need at least 2 timepoints with dates or timepoint markers\"\n        )\n    \n    # =========================================================================\n    # CHECK 7: Clinical Flags Review\n    # =========================================================================\n    items_with_clinical_flags = 0\n    total_clinical_flags = 0\n    critical_flags = []\n    \n    for item in data_items:\n        flags = item.get('clinical_flags', [])\n        if flags:\n            items_with_clinical_flags += 1\n            total_clinical_flags += len(flags)\n            \n            # Identify critical flags (severe, abnormal, etc.)\n            for flag in flags:\n                flag_lower = flag.lower()\n                if any(keyword in flag_lower for keyword in ['severe', 'abnormal', 'high risk', 'clinical attention']):\n                    critical_flags.append({\n                        'questionnaire': item.get('questionnaire', 'Unknown'),\n                        'timepoint': item.get('timepoint', '?'),\n                        'flag': flag\n                    })\n    \n    validation_report[\"data_quality_metrics\"][\"clinical_flags_summary\"] = {\n        \"items_with_flags\": items_with_clinical_flags,\n        \"total_flags\": total_clinical_flags,\n        \"critical_flags\": len(critical_flags),\n        \"critical_flags_details\": critical_flags[:10]  # Show first 10\n    }\n    \n    if critical_flags:\n        validation_report[\"recommendations\"].append(\n            f\"{len(critical_flags)} critical clinical flags detected - review before LLM processing\"\n        )\n    \n    # =========================================================================\n    # FINAL STATUS DETERMINATION\n    # =========================================================================\n    if validation_report[\"errors\"]:\n        validation_report[\"status\"] = \"FAIL\"\n        validation_report[\"recommendations\"].append(\n            \"Fix errors before proceeding to trend analysis or LLM processing\"\n        )\n    elif validation_report[\"warnings\"]:\n        if validation_report[\"status\"] != \"FAIL\":\n            validation_report[\"status\"] = \"WARNING\"\n        validation_report[\"recommendations\"].append(\n            \"Review warnings - data may still be usable but quality could be improved\"\n        )\n    \n    # Add summary\n    validation_report[\"summary\"] = {\n        \"total_items\": len(data_items),\n        \"items_with_all_required_fields\": items_with_all_required,\n        \"questionnaires_analyzed\": len(questionnaire_groups),\n        \"ready_for_trend_analysis\": can_analyze_trends,\n        \"critical_issues\": len(validation_report[\"errors\"]),\n        \"warnings\": len(validation_report[\"warnings\"]),\n        \"status\": validation_report[\"status\"]\n    }\n    \n    return validation_report\n\ndef format_validation_report(validation: Dict[str, Any]) -> str:\n    \"\"\"Format validation report as human-readable text\"\"\"\n    lines = []\n    lines.append(\"=\" * 70)\n    lines.append(\"DATA QUALITY VALIDATION REPORT\")\n    lines.append(\"=\" * 70)\n    lines.append(f\"Status: {validation['status']}\")\n    lines.append(f\"Total Items: {validation['total_items']}\")\n    lines.append(\"\")\n    \n    # Summary\n    if validation.get('summary'):\n        lines.append(\"SUMMARY:\")\n        for key, value in validation['summary'].items():\n            lines.append(f\"  {key}: {value}\")\n        lines.append(\"\")\n    \n    # Errors\n    if validation.get('errors'):\n        lines.append(\"ERRORS:\")\n        for error in validation['errors']:\n            lines.append(f\"  ❌ {error}\")\n        lines.append(\"\")\n    \n    # Warnings\n    if validation.get('warnings'):\n        lines.append(\"WARNINGS:\")\n        for warning in validation['warnings']:\n            lines.append(f\"  ⚠️  {warning}\")\n        lines.append(\"\")\n    \n    # Recommendations\n    if validation.get('recommendations'):\n        lines.append(\"RECOMMENDATIONS:\")\n        for rec in validation['recommendations']:\n            lines.append(f\"  💡 {rec}\")\n        lines.append(\"\")\n    \n    lines.append(\"=\" * 70)\n    return \"\\n\".join(lines)\n\n# =============================================================================\n# n8n CODE NODE EXECUTION (Direct execution - no function wrappers)\n# =============================================================================\n# Note: In n8n, 'items' is a global variable provided by the platform\n# This validator node should be placed AFTER the Preprocessor and BEFORE the Trend Analyzer\n\n# Check if running in n8n environment\nif 'items' in globals():\n    try:\n        # Debug: Log what we received from preprocessor\n        print(f\"🔍 VALIDATOR: Received {len(items)} items from preprocessor\")\n        \n        # Run validation\n        validation_result = validate_preprocessed_data(items)\n        \n        # Print formatted report\n        print(\"\\n\" + format_validation_report(validation_result))\n        \n        # Determine what to output based on validation status\n        if validation_result['status'] == 'FAIL':\n            print(f\"❌ VALIDATION FAILED: {len(validation_result['errors'])} critical error(s) found\")\n            print(\"🛑 Stopping workflow - fix errors before proceeding\")\n            \n            # Return validation report only (not the data)\n            return [{'json': {\n                'validation_status': 'FAIL',\n                'validation_report': validation_result,\n                'data_passed': False,\n                'message': 'Data quality check failed - see validation_report for details'\n            }}]\n        \n        elif validation_result['status'] == 'WARNING':\n            print(f\"⚠️  VALIDATION PASSED WITH WARNINGS: {len(validation_result['warnings'])} warning(s)\")\n            print(\"✅ Proceeding with data - review warnings\")\n            \n            # Return both validation report AND original data\n            return [{'json': {\n                'validation_status': 'WARNING',\n                'validation_report': validation_result,\n                'data_passed': True,\n                'message': 'Data quality check passed with warnings',\n                'processed_data': [item for item in items]  # Pass through original data\n            }}]\n        \n        else:  # PASS\n            print(f\"✅ VALIDATION PASSED: All checks successful\")\n            print(f\"   → {validation_result['summary']['questionnaires_analyzed']} questionnaires validated\")\n            print(f\"   → Ready for trend analysis: {validation_result['data_quality_metrics']['trend_analysis_readiness']['ready_for_trend_analysis']}\")\n            \n            # Return both validation report AND original data\n            return [{'json': {\n                'validation_status': 'PASS',\n                'validation_report': validation_result,\n                'data_passed': True,\n                'message': 'Data quality check passed - ready for trend analysis',\n                'processed_data': [item for item in items]  # Pass through original data\n            }}]\n\n    except Exception as e:\n        # Return detailed error information for n8n debugging\n        import traceback\n        \n        error_details = {\n            'error_message': str(e),\n            'error_type': type(e).__name__,\n            'input_items_count': len(items) if 'items' in globals() else 0,\n            'traceback': traceback.format_exc(),\n            'debug_info': {\n                'items_available': 'items' in globals(),\n                'items_type': type(items).__name__ if 'items' in globals() else 'undefined',\n                'help': 'Check that this Validator node is connected after the Questionnaire Preprocessor node'\n            }\n        }\n        \n        print(f\"❌ VALIDATOR ERROR: {str(e)}\")\n        print(f\"🔍 VALIDATOR DEBUG: Error details logged in output\")\n        \n        # Return error as JSON for next node\n        return [{'json': error_details}]\n\n"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        1872,
        0
      ],
      "id": "0ce0d084-b575-4d65-a1eb-cd448dc0cf9f",
      "name": "data checkpoint"
    },
    {
      "parameters": {
        "modelId": {
          "__rl": true,
          "value": "models/gemini-2.5-flash",
          "mode": "list",
          "cachedResultName": "models/gemini-2.5-flash"
        },
        "messages": {
          "values": [
            {
              "content": "Role: You are a helpful assistant tasked with synthesizing clinical questionnaire data into a clear and concise summary for a patient's profile.\n"
            },
            {}
          ]
        },
        "options": {}
      },
      "type": "@n8n/n8n-nodes-langchain.googleGemini",
      "typeVersion": 1,
      "position": [
        1232,
        176
      ],
      "id": "e8823a4d-899c-48f7-8ccc-2ff2d8b9b562",
      "name": "Message a model",
      "credentials": {
        "googlePalmApi": {
          "id": "CdqQrcdtTQy1cPpA",
          "name": "Google Gemini(PaLM) Api account"
        }
      }
    },
    {
      "parameters": {
        "operation": "appendOrUpdate",
        "documentId": {
          "__rl": true,
          "value": "1TnEmQ_7X_GxtzpaNzGO8agySxCCQi5KziqG7qkGelZc",
          "mode": "list",
          "cachedResultName": "n8n_process_data",
          "cachedResultUrl": "https://docs.google.com/spreadsheets/d/1TnEmQ_7X_GxtzpaNzGO8agySxCCQi5KziqG7qkGelZc/edit?usp=drivesdk"
        },
        "sheetName": {
          "__rl": true,
          "value": "Sheet1",
          "mode": "name"
        },
        "columns": {
          "mappingMode": "autoMapInputData",
          "value": {},
          "matchingColumns": [],
          "schema": [
            {
              "id": "questionnaire",
              "displayName": "questionnaire",
              "required": false,
              "defaultMatch": false,
              "display": true,
              "type": "string",
              "canBeUsedToMatch": true,
              "removed": false
            },
            {
              "id": "timepoint",
              "displayName": "timepoint",
              "required": false,
              "defaultMatch": false,
              "display": true,
              "type": "string",
              "canBeUsedToMatch": true,
              "removed": false
            },
            {
              "id": "date",
              "displayName": "date",
              "required": false,
              "defaultMatch": false,
              "display": true,
              "type": "string",
              "canBeUsedToMatch": true,
              "removed": false
            },
            {
              "id": "raw_total",
              "displayName": "raw_total",
              "required": false,
              "defaultMatch": false,
              "display": true,
              "type": "string",
              "canBeUsedToMatch": true,
              "removed": false
            },
            {
              "id": "scale_info",
              "displayName": "scale_info",
              "required": false,
              "defaultMatch": false,
              "display": true,
              "type": "string",
              "canBeUsedToMatch": true,
              "removed": false
            },
            {
              "id": "severity",
              "displayName": "severity",
              "required": false,
              "defaultMatch": false,
              "display": true,
              "type": "string",
              "canBeUsedToMatch": true,
              "removed": false
            },
            {
              "id": "clinical_flags",
              "displayName": "clinical_flags",
              "required": false,
              "defaultMatch": false,
              "display": true,
              "type": "string",
              "canBeUsedToMatch": true,
              "removed": false
            },
            {
              "id": "derived",
              "displayName": "derived",
              "required": false,
              "defaultMatch": false,
              "display": true,
              "type": "string",
              "canBeUsedToMatch": true,
              "removed": false
            },
            {
              "id": "responses",
              "displayName": "responses",
              "required": false,
              "defaultMatch": false,
              "display": true,
              "type": "string",
              "canBeUsedToMatch": true,
              "removed": false
            },
            {
              "id": "free_text",
              "displayName": "free_text",
              "required": false,
              "defaultMatch": false,
              "display": true,
              "type": "string",
              "canBeUsedToMatch": true,
              "removed": false
            },
            {
              "id": "who5_index",
              "displayName": "who5_index",
              "required": false,
              "defaultMatch": false,
              "display": true,
              "type": "string",
              "canBeUsedToMatch": true,
              "removed": false
            }
          ],
          "attemptToConvertTypes": false,
          "convertFieldsToString": false
        },
        "options": {
          "cellFormat": "RAW"
        }
      },
      "type": "n8n-nodes-base.googleSheets",
      "typeVersion": 4.7,
      "position": [
        1456,
        0
      ],
      "id": "9ed94d3e-98ff-477d-a0f5-3bb8456e360e",
      "name": "Append or update row in sheet",
      "credentials": {
        "googleSheetsOAuth2Api": {
          "id": "ckxLlSUPjGPkf4qH",
          "name": "Google Sheets account"
        }
      }
    },
    {
      "parameters": {
        "jsCode": "// Compact preprocessor for Gemini summary node in n8n (JavaScript)\n//\n// Input: items from n8n_questionnaire_preprocessor:\n//   [{ json: { questionnaire, timepoint, date, raw_total, severity, clinical_flags, derived, free_text, ... } }, ...]\n//\n// Output: one item with:\n//   { json: { summary: { timepoints: [ { tp, date, q: [ { name, tot, sev, scale?, flags?, note? }, ... ] }, ... ] } } }\n\nfunction safeStr(v) {\n  return v == null ? '' : String(v);\n}\n\nfunction safeInt(v) {\n  if (v == null || v === '') return 0;\n  const n = Number(v);\n  return Number.isFinite(n) ? Math.trunc(n) : 0;\n}\n\nfunction buildCompactOverview(items) {\n  // Groups all questionnaires by (timepoint, date)\n  const timepointMap = {}; // key: \"tp|date\" -> { tp, date, q: [] }\n\n  for (const item of items) {\n    const data = (item && item.json) || {};\n\n    const tp = safeInt(data.timepoint ?? 0);\n    const date = safeStr(data.date ?? '');\n    const name = safeStr(data.questionnaire ?? '').trim();\n\n    if (!name) continue;\n\n    const tpKey = `${tp}|${date}`;\n    if (!timepointMap[tpKey]) {\n      timepointMap[tpKey] = {\n        tp,\n        date,\n        q: []\n      };\n    }\n\n    const derived = data.derived || {};\n\n    const qEntry = {\n      name,\n      tot: safeInt(data.raw_total ?? derived.total_score),\n      sev: safeStr(data.severity ?? '')\n    };\n\n    // Short scale descriptor\n    const scaleInfo = safeStr(derived.scale ?? '');\n    if (scaleInfo) {\n      qEntry.scale = scaleInfo.slice(0, 80);\n    }\n\n    // Clinical flags (up to 4, each truncated)\n    const flags = Array.isArray(data.clinical_flags) ? data.clinical_flags : [];\n    if (flags.length > 0) {\n      const compactFlags = [];\n      for (const f of flags.slice(0, 4)) {\n        let s = safeStr(f);\n        if (s.length > 80) {\n          s = s.slice(0, 77) + '...';\n        }\n        compactFlags.push(s);\n      }\n      if (compactFlags.length > 0) {\n        qEntry.flags = compactFlags;\n      }\n    }\n\n    // Short free-text note\n    const freeText = safeStr(data.free_text ?? '');\n    if (freeText) {\n      qEntry.note = freeText.slice(0, 160);\n    }\n\n    timepointMap[tpKey].q.push(qEntry);\n  }\n\n  // Convert map to list, sorted by timepoint then date\n  const timepoints = Object.values(timepointMap).sort((a, b) => {\n    const tpDiff = safeInt(a.tp) - safeInt(b.tp);\n    if (tpDiff !== 0) return tpDiff;\n    return safeStr(a.date).localeCompare(safeStr(b.date));\n  });\n\n  return { timepoints };\n}\n\nfunction preprocessForGemini(items) {\n  const summary = buildCompactOverview(items);\n  return [\n    {\n      json: {\n        summary\n      }\n    }\n  ];\n}\n\n// n8n entrypoint\nreturn preprocessForGemini(items);"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        832,
        0
      ],
      "id": "541a9a58-aa16-4df4-bb97-7f24cbba04ec",
      "name": "prepare_for_gemini_ in JavaScript"
    }
  ],
  "pinData": {},
  "connections": {
    "When clicking ‘Execute workflow’": {
      "main": [
        [
          {
            "node": "Download file",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Download file": {
      "main": [
        [
          {
            "node": "Extract from File",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Extract from File": {
      "main": [
        [
          {
            "node": "data preprocess",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "data preprocess": {
      "main": [
        [
          {
            "node": "Append or update row in sheet",
            "type": "main",
            "index": 0
          },
          {
            "node": "prepare_for_gemini_ in JavaScript",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Append or update row in sheet": {
      "main": [
        [
          {
            "node": "data checkpoint",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Message a model": {
      "main": [
        []
      ]
    },
    "prepare_for_gemini_ in JavaScript": {
      "main": [
        [
          {
            "node": "Message a model",
            "type": "main",
            "index": 0
          }
        ]
      ]
    }
  },
  "active": false,
  "settings": {
    "executionOrder": "v1"
  },
  "versionId": "b9b86ed1-82d4-4a46-b76c-21f03b2e7a6d",
  "meta": {
    "templateCredsSetupCompleted": true,
    "instanceId": "1231ed3473ce02674fc8f3aa87e097ffd895878501597f7527fec7bcb3e4868d"
  },
  "id": "Oht3LcrRxCgzlVMT",
  "tags": []
}